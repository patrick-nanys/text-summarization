{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep-learning-hw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CLvBco2J-xK"
      },
      "source": [
        "## Deep learning homework - Milestone 1\n",
        "#####Topic: NLP - text summarization\n",
        "#####Authors: Bence Halasz, Patrick Nanys, Mate Jakab (Goal Diggers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGZDgfz_a6HG"
      },
      "source": [
        "###1. Data load from xlsx \n",
        "\n",
        "> Source: https://www.kaggle.com/shashichander009/inshorts-news-data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAEriAFohCba",
        "outputId": "e8cd746f-35b8-4ee9-fbed-0df562188a4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "xls = pd.read_excel(\"Inshorts Cleaned Data.xlsx\")\n",
        "# Load articles, stories\n",
        "input_raw = xls['Short']\n",
        "# Load headlines for articles and stories\n",
        "output_raw = xls['Headline']\n",
        "\n",
        "# Show example\n",
        "print(input_raw.head(), output_raw.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    The CBI on Saturday booked four former officia...\n",
            "1    Chief Justice JS Khehar has said the Supreme C...\n",
            "2    At least three people were killed, including a...\n",
            "3    Mukesh Ambani-led Reliance Industries (RIL) wa...\n",
            "4    TV news anchor Arnab Goswami has said he was t...\n",
            "Name: Short, dtype: object 0    4 ex-bank officials booked for cheating bank o...\n",
            "1       Supreme Court to go paperless in 6 months: CJI\n",
            "2    At least 3 killed, 30 injured in blast in Sylh...\n",
            "3    Why has Reliance been barred from trading in f...\n",
            "4    Was stopped from entering my own studio at Tim...\n",
            "Name: Headline, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0CyGJ3cgoT0"
      },
      "source": [
        "###2. Taking out stopwords and shorten words (optional)\n",
        "> Shortening options: lemmatizing/stemmimg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCwWTYGhymGQ",
        "outputId": "b706c546-8b33-4da2-f300-130cf6bd7e74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# nltk library and used modules\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Lemmatize with POS Tag (a given word gets the right POS tag)\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXMYhpsWncrC",
        "outputId": "ed7b93d2-718f-4ab5-d0b9-c7d12525e5e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print('Lemmatizing example')\n",
        "\n",
        "words = [\"dogs\", \"are\", \"better\", \"than\", \"cats\"] \n",
        "for w in words:\n",
        "    print(w, \"\\t->\", WordNetLemmatizer().lemmatize(w, get_wordnet_pos(w)))\n",
        "\n",
        "print('\\nStemming example')\n",
        "\n",
        "words = [\"run\", \"ran\", \"runner\", \"running\"] \n",
        "for w in words: \n",
        "    print(w, \"->\", PorterStemmer().stem(w)) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemmatizing example\n",
            "dogs \t-> dog\n",
            "are \t-> be\n",
            "better \t-> well\n",
            "than \t-> than\n",
            "cats \t-> cat\n",
            "\n",
            "Stemming example\n",
            "run -> run\n",
            "ran -> ran\n",
            "runner -> runner\n",
            "running -> run\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTMaCpj6iR8J"
      },
      "source": [
        "def takeoutstopwords_and_shorten(data, mode='none'):\n",
        "    data_nltk = []\n",
        "\n",
        "    # nltk's built in lemmatizer, stemmer and detokenizer\n",
        "    lemmatizer = WordNetLemmatizer()  \n",
        "    ps = PorterStemmer()\n",
        "    detok = TreebankWordDetokenizer()\n",
        "\n",
        "    # Cycle through data (range set to 10 instead of data.size until we use this)\n",
        "    for i in range(data.size):  #range(data.size)\n",
        "        # Tokenize\n",
        "        tokenized = word_tokenize(data[i])\n",
        "        # Take out stopwords\n",
        "        without_stopword = [word for word in tokenized if not word in stopwords.words('english')]\n",
        "        # Shorten if parameter 'mode' is 'lemmatize' or 'stem'\n",
        "        shorter = []\n",
        "        for word in without_stopword: \n",
        "            if mode == 'lemmatize':\n",
        "                shorter.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
        "            elif mode == 'stem':\n",
        "                shorter.append(ps.stem(word))\n",
        "            else:\n",
        "                shorter.append(word)\n",
        "        # Detokenize\n",
        "        back_to_string = detok.detokenize(shorter)\n",
        "\n",
        "        data_nltk.append(back_to_string)\n",
        "\n",
        "    return data_nltk"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtQL4PzhsL3-",
        "outputId": "36458347-5517-4d89-a744-1a24838055b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Take out stopwords and shorten input_raw data with lemmatize and stem\n",
        "input_nltk_lemma = takeoutstopwords_and_shorten(input_raw, mode='lemmatize')\n",
        "input_nltk_stem = takeoutstopwords_and_shorten(input_raw, mode='stem')\n",
        "\n",
        "#Maybe it is useless to take out stopwords from output value (it is already short)\n",
        "#output_nltk = takeoutstopwords_and_shorten(output_raw)\n",
        "\n",
        "i = 2\n",
        "print('Example\\n', input_raw[i], '\\n', input_nltk_lemma[i], '\\n', input_nltk_stem[i])\n",
        "\n",
        "#Not connected to tokenization yet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example\n",
            " At least three people were killed, including a policeman, while 30 others were wounded on Saturday evening in two explosions in Sylhet, Bangladesh. The explosions were targetted at people and police officials who were witnessing an over 30-hour-long gunfight between extremists and commandos. Earlier on Friday, a man had blown himself up in front of a checkpoint near Dhaka Airport. \n",
            " At least three people kill, include policeman , 30 others wound Saturday even two explosion Sylhet, Bangladesh . The explosion targetted people police official witness 30-hour-long gunfight extremist commando . Earlier Friday, man blown front checkpoint near Dhaka Airport. \n",
            " At least three peopl kill, includ policeman , 30 other wound saturday even two explos sylhet, bangladesh . the explos target peopl polic offici wit 30-hour-long gunfight extremist commando . earlier friday, man blown front checkpoint near dhaka airport.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn24taYItgm3"
      },
      "source": [
        "###3. Tokenization and padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5NArR5oguZC"
      },
      "source": [
        "##### Obtaining length data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVFDLC_pg0FQ",
        "outputId": "f2b02bf8-b603-4102-fd0c-e5d908c67b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "input_lengths = pd.Series([len(x) for x in input_raw])\n",
        "output_lengths = pd.Series([len(x) for x in output_raw])\n",
        "print('Inputs:\\n', input_lengths.describe())\n",
        "print('Outputs:\\n', output_lengths.describe())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs:\n",
            " count    55104.000000\n",
            "mean       362.374129\n",
            "std         24.985897\n",
            "min        280.000000\n",
            "25%        346.000000\n",
            "50%        363.000000\n",
            "75%        381.000000\n",
            "max        448.000000\n",
            "dtype: float64\n",
            "Outputs:\n",
            " count    55104.000000\n",
            "mean        50.016188\n",
            "std          6.461184\n",
            "min          8.000000\n",
            "25%         46.000000\n",
            "50%         48.000000\n",
            "75%         56.000000\n",
            "max         71.000000\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvKS-MbphCPK"
      },
      "source": [
        "# maxlen\n",
        "# taking values > and round figured to 75th percentile\n",
        "# at the same time not leaving high variance\n",
        "encoder_maxlen = 400\n",
        "decoder_maxlen = 75"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olKeNNn5hLnY"
      },
      "source": [
        "Adding start and stop signs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxQt7HvhhJ76"
      },
      "source": [
        "# should be applied to output\n",
        "def apply_start_end(data):\n",
        "  return data.apply(lambda x: '<START> ' + x + ' <END>')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF4knn3xhpj8"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I72VwUr7hsHG"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def tokenize(inputs, outputs):\n",
        "  # since < and > from default tokens cannot be removed\n",
        "  filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "  oov_token = '<UNK>'\n",
        "\n",
        "  input_tokenizer = Tokenizer(oov_token=oov_token)\n",
        "  output_tokenizer = Tokenizer(filters=filters, oov_token=oov_token)\n",
        "\n",
        "  input_tokenizer.fit_on_texts(inputs)\n",
        "  output_tokenizer.fit_on_texts(outputs)\n",
        "\n",
        "  tokenized_inputs = input_tokenizer.texts_to_sequences(inputs)\n",
        "  tokenized_outputs = output_tokenizer.texts_to_sequences(outputs)\n",
        "\n",
        "  return tokenized_inputs, tokenized_outputs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJiyIJYciIgT"
      },
      "source": [
        "#### Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3cQXKA3iJx2"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def add_padding(inputs, outputs, encoder_maxlen, decoder_maxlen):\n",
        "  padded_inputs = pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "  padded_outputs = pad_sequences(outputs, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
        "  return padded_inputs, padded_outputs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqLinn2u88J6"
      },
      "source": [
        "###4. Split dataset into train, validation and test datas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL15_ImQtNup"
      },
      "source": [
        "def dataset_split(X, Y, valid_split, test_split):\n",
        "  v_start = int(len(X)*(1-valid_split-test_split))\n",
        "  t_start = int(len(X)*(1-test_split))\n",
        "  X_train, Y_train = X[:v_start], Y[:v_start]\n",
        "  X_valid, Y_valid = X[v_start:t_start], Y[v_start:t_start]\n",
        "  X_test , Y_test  = X[t_start:], Y[t_start:]\n",
        "  return X_train, Y_train, X_valid, Y_valid, X_test, Y_test"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "papK44dU9hRk"
      },
      "source": [
        "###5. Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV2oR7no9lTr",
        "outputId": "89d3e298-9791-4529-fbc0-7bae68c01fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Example data preprocessing\n",
        "\n",
        "# replace unreadable part\n",
        "input_raw = input_raw.replace([\"&#39;\"], [\"'\"], regex=True)\n",
        "output_raw = output_raw.replace([\"&#39;\"], [\"'\"], regex=True)\n",
        "\n",
        "start_end_output = apply_start_end(output_raw)\n",
        "lemmatized_inputs = takeoutstopwords_and_shorten(input_raw, mode='lemmatize')\n",
        "tokenized_inputs, tokenized_outputs = tokenize(lemmatized_inputs, start_end_output)\n",
        "X, Y = add_padding(tokenized_inputs, tokenized_outputs, encoder_maxlen, decoder_maxlen)\n",
        "X_train, Y_train, X_valid, Y_valid, X_test, Y_test = dataset_split(X, Y, valid_split=0.2, test_split=0.1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10min, sys: 47.5 s, total: 10min 47s\n",
            "Wall time: 10min 48s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsJvM3ZsYrkJ",
        "outputId": "c0131578-55ef-410d-9b32-17f143d56643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape, X_test.shape, Y_test.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((38572, 400), (38572, 75), (11021, 400), (11021, 75), (5511, 400), (5511, 75))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AjkbDf1aHlB",
        "outputId": "e35ae448-3bc2-4f8e-8fb8-dc2020928cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape[0] + X_valid.shape[0] + X_test.shape[0]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}